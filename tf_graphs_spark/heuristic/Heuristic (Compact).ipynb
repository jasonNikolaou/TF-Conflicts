{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69699e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from time import time\n",
    "from dsd import *\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80529530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter GURO_PAR_SPECIAL\n",
      "Set parameter TokenServer to value \"sccsvc\"\n",
      "number of nodes: 10000\n",
      "number of (conflict) edges: 49499913\n",
      "number of friend edges: 495087\n",
      "number of projects: 100\n"
     ]
    }
   ],
   "source": [
    "input_file = './synth.pickle'\n",
    "pipageSetting = \"randomized\"\n",
    "\n",
    "env = gp.Env(empty=True)\n",
    "# env.setParam(\"Threads\", 16)\n",
    "# env.setParam(\"OutputFlag\",0) # suppress gurobi console output\n",
    "env.start()\n",
    "timeLimit = 1000\n",
    "\n",
    "times = dict() # dictionary to keep time for different stages\n",
    "start = time()\n",
    "# Read graph\n",
    "with open(input_file, 'rb') as file:\n",
    "    projects = pickle.load(file)\n",
    "    numOfProjects = len(projects)\n",
    "    node_capacities = pickle.load(file)\n",
    "    task_capacities = pickle.load(file)\n",
    "    c = pickle.load(file)\n",
    "    w = pickle.load(file)\n",
    "    edges = pickle.load(file)\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "G_c = nx.complement(G) # friends graph\n",
    "nodes = list(G.nodes)\n",
    "n = len(nodes)\n",
    "\n",
    "# print(f'projects = {projects}')\n",
    "print(f'number of nodes: {n}')\n",
    "print(f'number of (conflict) edges: {len(edges)}')\n",
    "print(f'number of friend edges: {len(G_c.edges())}')\n",
    "print(f'number of projects: {len(projects)}')\n",
    "times[\"read_input\"] = time() - start\n",
    "# nx.draw(G_c, pos=nx.spring_layout(G_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f0a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def copy_solution(X):\n",
    "    x = dict()\n",
    "    for u in new_nodes:\n",
    "        for p in projects:\n",
    "            x[(u, p)] = X[(u, p)].X\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fb78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning the friend graph ...\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "print('Partitioning the friend graph ...')\n",
    "# Get adjacency-matrix as numpy-array\n",
    "adj_mat = nx.to_numpy_array(G_c)\n",
    "\n",
    "# Partition\n",
    "sc = SpectralClustering(10, affinity='precomputed', n_init=1000)\n",
    "sc.fit(adj_mat)\n",
    "partitions = dict()\n",
    "for i, n in zip(sc.labels_, G_c.nodes()):\n",
    "    if i in partitions:\n",
    "        partitions[i].append(n)\n",
    "    else:\n",
    "        partitions[i] = [n]\n",
    "        \n",
    "new_nodes = []\n",
    "superNodeSize = dict()\n",
    "for part in partitions.values():\n",
    "    superNode = ','.join(part)\n",
    "    new_nodes.append(superNode)\n",
    "    superNodeSize[superNode] = len(part)\n",
    "\n",
    "print('Setting up the compact problem ...')\n",
    "Gc_compact = nx.Graph()\n",
    "Gc_compact.add_nodes_from(new_nodes)\n",
    "G_compact = nx.complement(Gc_compact)\n",
    "w_compact = dict()\n",
    "for e in G_compact.edges():\n",
    "    u, v = e\n",
    "    w_compact[e] = superNodeSize[u] * superNodeSize[v]\n",
    "    \n",
    "c_compact = dict()\n",
    "for superNode in new_nodes:\n",
    "    for p in projects:\n",
    "        c_compact[(superNode, p)] = 0\n",
    "\n",
    "for superNode in new_nodes:\n",
    "    simple_nodes = superNode.split(',')\n",
    "    for u in simple_nodes:\n",
    "        for p in projects:\n",
    "            if (u, p) in c:\n",
    "                c_compact[(superNode, p)] += c[(u, p)]\n",
    "times['compaction'] = time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17366c0a",
   "metadata": {},
   "source": [
    "## Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf286779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time()\n",
    "# print('Removing isolated nodes ...')\n",
    "# # remove isolated nodes\n",
    "# H = G_c # copy of friends graph\n",
    "# print(f'# of friend edges = {len(H.edges())}')\n",
    "# superNodeSize = dict()\n",
    "# isolated_nodes = [node for node in H.nodes() if H.degree(node) == 0]\n",
    "# new_nodes = isolated_nodes\n",
    "# for node in new_nodes:\n",
    "#     superNodeSize[node] = 1\n",
    "# H.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# print('Finding connected components ...')\n",
    "# # find connected components\n",
    "# connected_comps = []\n",
    "# for comp in list(nx.connected_components(H)):\n",
    "#     if len(comp) <= 5: # compacting small components\n",
    "#         H.remove_nodes_from(list(comp))\n",
    "#         super_node = ','.join(list(comp))\n",
    "#         superNodeSize[super_node] = len(list(comp))\n",
    "#         new_nodes.append(super_node)\n",
    "#     else:\n",
    "#         connected_comps.append(H.subgraph(list(comp)))\n",
    "        \n",
    "# print('Finding dense subgraphs ...')\n",
    "# # Find dense subgraphs in each connected component\n",
    "# for cc in connected_comps:\n",
    "#     cc_tmp = nx.Graph()\n",
    "#     cc_tmp.add_nodes_from(cc.nodes)\n",
    "#     cc_tmp.add_edges_from(cc.edges)\n",
    "#     while cc_tmp.nodes:  \n",
    "#         greedy_R = exact_densest(cc_tmp)\n",
    "# #         greedy_r = exact_densest(cc_tmp)\n",
    "#         super_node = \",\".join(greedy_R[0])\n",
    "#         superNodeSize[super_node] = len(greedy_R[0])\n",
    "#         new_nodes.append(super_node)\n",
    "#         cc_tmp.remove_nodes_from(greedy_R[0])\n",
    "        \n",
    "# print('Setting up the compact problem ...')\n",
    "# Gc_compact = nx.Graph()\n",
    "# Gc_compact.add_nodes_from(new_nodes)\n",
    "# G_compact = nx.complement(Gc_compact)\n",
    "# w_compact = dict()\n",
    "# for e in G_compact.edges():\n",
    "#     u, v = e\n",
    "#     w_compact[e] = superNodeSize[u] * superNodeSize[v]\n",
    "    \n",
    "# c_compact = dict()\n",
    "# for superNode in new_nodes:\n",
    "#     for p in projects:\n",
    "#         c_compact[(superNode, p)] = 0\n",
    "\n",
    "# for superNode in new_nodes:\n",
    "#     simple_nodes = superNode.split(',')\n",
    "#     for u in simple_nodes:\n",
    "#         for p in projects:\n",
    "#             if (u, p) in c:\n",
    "#                 c_compact[(superNode, p)] += c[(u, p)]\n",
    "# times['compaction'] = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941adcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# of (compact) conflict edges = {len(G_compact.edges)}')\n",
    "print(f'# of (compact) friend edges = {len(Gc_compact.edges)}')\n",
    "print(f'# of (compact) nodes = {len(G_compact.nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "superNodeSize.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e52d2",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def teams_to_x(teams):\n",
    "    x = dict()\n",
    "    for t in teams:\n",
    "        team = teams[t]\n",
    "        for u in team:\n",
    "            x[(u, t)] = 1\n",
    "    return x\n",
    "\n",
    "def baselines():\n",
    "    # Random\n",
    "    teams_random = {t: [] for t in random.sample(projects, len(projects))}\n",
    "    for u in random.sample(nodes, len(nodes)):\n",
    "        for t in random.sample(teams_random.keys(), len(teams_random)):\n",
    "            if len(teams_random[t]) < task_capacities[t]:\n",
    "                teams_random[t].append(u)\n",
    "    x_random = teams_to_x(teams_random)\n",
    "    \n",
    "#     # Greedy\n",
    "#     teams_greedy = {t: [] for t in projects}\n",
    "#     for u in nodes:\n",
    "#         t_max = -1\n",
    "#         increase = -2**60\n",
    "#         for t in teams_greedy:\n",
    "#             x_greedy = teams_to_x(teams_greedy)\n",
    "#             f_greedy = f(x_greedy)\n",
    "#             x_tmp = teams_to_x(teams_greedy)\n",
    "#             if len(teams_greedy[t]) < capacities[t]:\n",
    "#                 x_tmp[(u, t)] = 1\n",
    "#                 inc = f(x_tmp) - f_greedy\n",
    "#                 if inc > increase:\n",
    "#                     increase = inc\n",
    "#                     t_max = t\n",
    "#         teams_greedy[t_max].append(u)\n",
    "#     x_greedy = teams_to_x(teams_greedy)\n",
    "    \n",
    "#     return x_random, x_greedy\n",
    "    return x_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe741a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51746641",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.001\n",
    "\n",
    "# Define objective function\n",
    "def f(x):\n",
    "    res = 0\n",
    "    # project preference term\n",
    "    for u in nodes:\n",
    "        for p in projects:\n",
    "            if (u, p) in c and (u, p) in x:\n",
    "                res += c[(u, p)] * x[(u, p)]\n",
    "    res *= lambda_\n",
    "    \n",
    "    # conflict term\n",
    "    for e in edges:\n",
    "        u, v = e\n",
    "        inner_sum = 0\n",
    "        for p in projects:\n",
    "            if (u, p) in x and (v, p) in x:\n",
    "                inner_sum += x[(u, p)] * x[(v, p)]\n",
    "        res += (1 - inner_sum)\n",
    "    return res\n",
    "\n",
    "# Pipage rounding\n",
    "def construct_graph(x):\n",
    "    H = nx.Graph()\n",
    "    H.add_nodes_from(nodes, bipartite=0)\n",
    "    H.add_nodes_from(projects, bipartite=1)\n",
    "    edges = [(u, p) for u in nodes for p in projects if (u, p) in x and not x[(u, p)] < eps and not x[(u, p)] > 1 - eps]\n",
    "    H.add_edges_from(edges)\n",
    "    return H\n",
    "\n",
    "def find_cycle(H):\n",
    "    try:\n",
    "        cycle = nx.find_cycle(H)\n",
    "    except Exception:\n",
    "        cycle = []\n",
    "    return cycle\n",
    "\n",
    "def find_path(graph):\n",
    "    def dfs_with_backtracking(vertex, path):\n",
    "        nonlocal max_path\n",
    "        path.append(vertex)\n",
    "\n",
    "        for neighbor in graph[vertex]:\n",
    "            if neighbor not in path:\n",
    "                dfs_with_backtracking(neighbor, path)\n",
    "\n",
    "        if len(path) > len(max_path):\n",
    "            max_path = path.copy()\n",
    "\n",
    "        path.pop()\n",
    "\n",
    "    max_path = []\n",
    "    for start_vertex in graph.nodes:\n",
    "        dfs_with_backtracking(start_vertex, [])\n",
    "        if max_path:\n",
    "            return list(zip(max_path, max_path[1:]))\n",
    "    \n",
    "    print(\"No path found...\")\n",
    "    return max_path\n",
    "\n",
    "def format_path(R):\n",
    "    R_new = deque()\n",
    "    for e in R:\n",
    "        u, p = e if e[0] in nodes else tuple(reversed(e)) # (student, project) edge\n",
    "        R_new.append((u, p))\n",
    "    return list(R_new)\n",
    "\n",
    "# def calc_eps(x, R):\n",
    "#     # Divide R into M1 and M2 matchings\n",
    "#     M1 = R[::2]\n",
    "#     M2 = R[1::2]\n",
    "    \n",
    "#     # Calculate eps1, eps2\n",
    "#     eps1 = min(min([x[e] for e in M1]), min([1 - x[e] for e in M2]))\n",
    "#     eps2 = min(min([1 - x[e] for e in M1]), min([x[e] for e in M2]))\n",
    "    \n",
    "#     return eps1, eps2, M1, M2\n",
    "\n",
    "def remove_dec_error(x):            \n",
    "    #only keep the keys corresponding to value 1\n",
    "    x_new = dict()\n",
    "    for e in x:\n",
    "        if x[e] >  1 - eps:\n",
    "            x_new[e] = 1\n",
    "    return x_new\n",
    "\n",
    "# def step(x, eps, M1, M2):\n",
    "#     x_new = x.copy()\n",
    "#     for e in M1:\n",
    "#         x_new[e] += eps\n",
    "#     for e in M2:\n",
    "#         x_new[e] -= eps\n",
    "#     return x_new\n",
    "\n",
    "\n",
    "# def rand_round(x, eps1, eps2, M1, M2):\n",
    "#     rand = random.uniform(0, 1)\n",
    "#     if rand < eps1 / (eps1 + eps2):\n",
    "# #         x_new = step(x, -eps1, M1, M2)\n",
    "#         eps = -eps1\n",
    "#         for e in M1:\n",
    "#             x[e] += eps\n",
    "#         for e in M2:\n",
    "#             x[e] -= eps\n",
    "#     else:\n",
    "# #         x_new = step(x, eps2, M1, M2)\n",
    "#         eps = eps2\n",
    "#         for e in M1:\n",
    "#             x[e] += eps\n",
    "#         for e in M2:\n",
    "#             x[e] -= eps\n",
    "#     return x\n",
    "    \n",
    "# def clean_edges(x, H):\n",
    "#     integral_edges = [e for e in H.edges if np.isclose(x[e], 0) or np.isclose(x[e], 1)]\n",
    "#     H.remove_edges_from(integral_edges)\n",
    "#     return H\n",
    "\n",
    "def rand_pipage_help(x, R):\n",
    "    R = format_path(R)\n",
    "#     eps1, eps2, M1, M2 = calc_eps(x, R)\n",
    "    # Divide R into M1 and M2 matchings\n",
    "    M1 = R[::2]\n",
    "    M2 = R[1::2]\n",
    "\n",
    "    # Calculate eps1, eps2\n",
    "    eps1 = min(min([x[e] for e in M1]), min([1 - x[e] for e in M2]))\n",
    "    eps2 = min(min([1 - x[e] for e in M1]), min([x[e] for e in M2]))\n",
    "    \n",
    "#     x_new = rand_round(x, eps1, eps2, M1, M2)\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand < eps1 / (eps1 + eps2):\n",
    "#         x_new = step(x, -eps1, M1, M2)\n",
    "        eps = -eps1\n",
    "        for e in M1:\n",
    "            x[e] += eps\n",
    "        for e in M2:\n",
    "            x[e] -= eps\n",
    "    else:\n",
    "#         x_new = step(x, eps2, M1, M2)\n",
    "        eps = eps2\n",
    "        for e in M1:\n",
    "            x[e] += eps\n",
    "        for e in M2:\n",
    "            x[e] -= eps\n",
    "    return x\n",
    "\n",
    "def pipage(x, setting=\"deterministic\"):\n",
    "    H = construct_graph(x)\n",
    "    print(f'# of edges of H = {len(H.edges())}')\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        if i%100 == 0:\n",
    "            print(f'iteration = {i}')\n",
    "        R = find_cycle(H)\n",
    "        R = R if R else find_path(H)\n",
    "        if R and len(R) > 1:\n",
    "            x = rand_pipage_help(x, R) # randomized\n",
    "#             clean_edges(x, H)\n",
    "            integral_edges = [e for e in H.edges if x[e] < eps or x[e] > 1-eps]\n",
    "            H.remove_edges_from(integral_edges)\n",
    "        else:\n",
    "            return remove_dec_error(x)\n",
    "        \n",
    "    print(\"Error: Pipage rounding failed ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91600c6e",
   "metadata": {},
   "source": [
    "## Solve compacted problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round34():\n",
    "    print(f'Running round34 ...')\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    # Create a new model\n",
    "    m = gp.Model(\"linear\") #, env=env)\n",
    "    m.Params.timeLimit = 300 #timeLimit\n",
    "\n",
    "    print('Creating variables ...')\n",
    "    # Create variables\n",
    "    X = dict()\n",
    "    for u in new_nodes:\n",
    "        for p in projects:\n",
    "            X[(u, p)] = m.addVar(lb=0, ub=1, vtype=GRB.CONTINUOUS)\n",
    "\n",
    "    print('Creating auxiliary variables ...')\n",
    "    # Auxiliary variables\n",
    "    Z = dict()\n",
    "    S = dict()\n",
    "    print(f'# of G_compact edges = {len(G_compact.edges())}')\n",
    "    print(f'# of nodes of G_compact = {len(G_compact.nodes())}')\n",
    "    for e in G_compact.edges():\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "        for t in projects:\n",
    "            Z[(u, v, t)] = m.addVar(vtype=GRB.CONTINUOUS)\n",
    "            S[(u, v, t)] = m.addVar(vtype=GRB.CONTINUOUS)\n",
    "            m.addConstr(S[(u, v, t)] == X[(u, t)] + X[(v, t)])\n",
    "            m.addConstr(Z[(u, v, t)] == gp.min_(S[(u, v, t)], constant = 1))\n",
    "\n",
    "    print('Adding constraints ...')   \n",
    "    # Add constraints\n",
    "    # Each student assigned to exactly one project\n",
    "    for u in new_nodes:\n",
    "        expr = gp.LinExpr(numOfProjects*[1], [X[(u, t)] for t in projects])\n",
    "        m.addConstr(expr == 1)\n",
    "\n",
    "    # Project capacity constraints\n",
    "    for p in projects:\n",
    "        expr = gp.LinExpr([superNodeSize[u] for u in new_nodes], [X[(u, p)] for u in new_nodes])\n",
    "        m.addConstr(expr <= task_capacities[p])\n",
    "\n",
    "    # Relaxed objective L(x)\n",
    "    L = gp.LinExpr()\n",
    "\n",
    "    # Linear (project preference) term\n",
    "    for u in new_nodes:\n",
    "        for p in projects:\n",
    "            if (u, p) in c_compact:\n",
    "                L += c_compact[(u, p)] * X[(u, p)]\n",
    "    L *= lambda_\n",
    "\n",
    "    # Max-cut term\n",
    "    for e in G_compact.edges():\n",
    "        u = e[0]; v = e[1]\n",
    "        for p in projects:\n",
    "            L += w_compact[(u, v)] * Z[(u, v, p)] \n",
    "            \n",
    "    L -= sum(w_compact.values())\n",
    "\n",
    "    m.setObjective(L, GRB.MAXIMIZE)    \n",
    "    times[\"relax_model34\"] = time() - start\n",
    "\n",
    "    print(f'Optimizing ...')\n",
    "    start = time()\n",
    "    # Optimize model\n",
    "    m.optimize()\n",
    "\n",
    "    # Convert solution to dictionary format\n",
    "    x_frac34 = copy_solution(X)\n",
    "    times[\"optimize_relaxation34\"] = time() - start\n",
    "\n",
    "    return x_frac34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951779aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mul = 10\n",
    "lambda_ = lambda_mul * sum(w_compact.values()) / len(nodes)\n",
    "x_frac34 = round34()\n",
    "# x_frac12 = round12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3964548",
   "metadata": {},
   "source": [
    "## Unroll solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(x_compact):\n",
    "    x_unroll = dict()\n",
    "    for e in x_compact:\n",
    "        superNode, p = e\n",
    "        simple_nodes = superNode.split(',')\n",
    "        for u in simple_nodes:\n",
    "            if x_compact[e] > eps:\n",
    "                x_unroll[(u, p)] = x_compact[e]\n",
    "    return x_unroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96676e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unroll34 = unroll(x_frac34)\n",
    "\n",
    "# x_unroll12 = unroll(x_frac12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_teams(x):\n",
    "    teams = dict()\n",
    "    for p in projects:\n",
    "        teams[p] = []\n",
    "\n",
    "    for e in x:\n",
    "        u, p = e\n",
    "        if x[e] == 1:\n",
    "            u, p = e\n",
    "            teams[p].append(u)\n",
    "            \n",
    "    for p in projects:\n",
    "        teams[p].sort()\n",
    "        \n",
    "    return teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f936a9fd",
   "metadata": {},
   "source": [
    "## Round solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3320ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rounding 3/4 solution ...')\n",
    "start = time()\n",
    "x_round34 = pipage(x_unroll34, 'randomized')\n",
    "times['rounding_34'] = time() - start\n",
    "# print('Rounding 1/2 solution ...')\n",
    "# start = time()\n",
    "# x_round12 = pipage(x_unroll12, 'deterministic')\n",
    "# times['rounding_12'] = time() - start\n",
    "print('Constructing teams ...')\n",
    "teams34 = construct_teams(x_round34)\n",
    "# teams12 = construct_teams(x_round12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_34 = f(x_round34)\n",
    "print(sum(times.values()) - times['read_input'])\n",
    "# val_12 = f(x_round12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36715c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_random = baselines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_random = f(x_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afc24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
